{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim            #提供optimizer \n",
    "import torch.utils.data as Data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "torch.set_printoptions(linewidth=120)            #display options for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()           # 输入x:(batch,3,32,32)\n",
    "        \n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),    # -> (batch,32,32,32)\n",
    "            nn.BatchNorm2d(32),              #只有block第一个conv2d用了normalize\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),     # -> (batch,64,32,32)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),              #只有block最后进行了池化      # -> (batch,64,16,16)\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),   # -> (batch,128,16,16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),   # -> (batch,128,16,16)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                    # -> (batch,128,8,8)\n",
    "            nn.Dropout2d(p=0.5),                         #只有第二个block用了dropout\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # -> (batch,256,8,8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),   # -> (batch,256,8,8)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                  # -> (batch,256,4,4)\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):                    #一直在对一个输入的数据t做操作          \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)           # x从(b,256,4,4)变成了(b,4096)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "BATCH_SIZE = 100\n",
    "EPOCH = 80\n",
    "LR = 0.001\n",
    "\n",
    "\n",
    "#network模型用GPU还是CPU训练,这里由于之前存入的network是CPU训练并保存的，所以这里用GPU会出问题\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,                                              \n",
    "    download=False,\n",
    "    transform=transforms.Compose([                 \n",
    "        transforms.RandomHorizontalFlip(),     #这句代码主要是用来做数据增强的，为了防止训练出现过拟合，\n",
    "                                               #通常在小型数据集上，通过随机翻转图片，随机调整图片的亮度，来达到增加训练时数据集的容量。\n",
    "        transforms.RandomGrayscale(),          #这句代码也是做数据增强的，依概率转化为灰度图，默认概率p=0.5\n",
    "        transforms.ToTensor(),                            #表示要将数据变成tensor\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),          #标准化\n",
    "    ])\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(  \n",
    "    train_set                                                #表示提取的数据集是train_set\n",
    "    ,batch_size=BATCH_SIZE                                        #表示批量大小为10,默认为0\n",
    "    ,shuffle=True                                            #表示打乱数据顺序\n",
    "    ,num_workers=2\n",
    ")\n",
    "\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def pred_rate(preds,labels):\n",
    "    return preds.eq(labels).sum().item()/labels.shape[0]\n",
    "\n",
    "# For updating learning rate, 设置lr逐渐变小，最后converge\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载之前保存的net\n",
    "network = Network().to(device)\n",
    "#torch.cuda.set_device(0)\n",
    "#import torch.backends.cudnn as cudnn\n",
    "#cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 2.00 GiB total capacity; 234.23 MiB already allocated; 1.29 GiB free; 55.77 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5ae8269a1901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                         \u001b[1;31m#使用backpropogation算法计算grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#表示将要用该optimizer，通过loss function的最小化来更新weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 2.00 GiB total capacity; 234.23 MiB already allocated; 1.29 GiB free; 55.77 MiB cached)"
     ]
    }
   ],
   "source": [
    "optimizer=optim.Adam(network.parameters(),lr=LR) #需要将network的参数(包括weight)传入才能构建optimizer,并且learning rate=0.001\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "curr_lr = LR\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step,(batch_x,batch_y) in enumerate(train_loader):\n",
    "        batch_x,batch_y=batch_x.to(device),batch_y.to(device)     #如果可以，tensor放到gpu上\n",
    "        \n",
    "        out=network(batch_x)\n",
    "        loss=loss_func(out,batch_y)            #F.cross_entropy函数计算loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                         #使用backpropogation算法计算grad\n",
    "        optimizer.step()    #表示将要用该optimizer，通过loss function的最小化来更新weights\n",
    "        \n",
    "        preds=out.argmax(dim=1)\n",
    "        \n",
    "        if (step+1)%100 == 0:\n",
    "             print('Epoch: ', epoch, '| Step: ', step+1,'| loss: ',loss.item(), '| prection rate: ',pred_rate(preds,batch_y))\n",
    "    \n",
    "    # Decay learning rate，每20个epoch的时候更新一次lr\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network,'cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data'                      \n",
    "    ,train=False                                           \n",
    "    ,download=False                                       \n",
    "    ,transform=transforms.Compose([                 \n",
    "        transforms.ToTensor(),                            #表示要将数据变成tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),          #标准化,数据变到了（-1，1）之间的标准值\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(  \n",
    "    test_set                                                #表示提取的数据集是train_set\n",
    "    ,batch_size=BATCH_SIZE                                        #表示批量大小为10,默认为0\n",
    "    ,shuffle=True                                            #表示打乱数据顺序\n",
    "    ,num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using the test set,the prediction rate is:  0.5937\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "# test,发现在测试集上表现不好，则这个模型有点过拟合了，可以再加入预训练，数据增强，需要更多的数据，或者更少的参数\n",
    "network=torch.load('cnn.pkl')\n",
    "network.eval()\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():         # 使用 torch,no_grad()构建不需要track的上下文环境，这个时候再不会跟踪track各个tensor的梯度\n",
    "    sum_correct = 0\n",
    "    total = 0\n",
    "    for test_x,test_y in test_loader:               #test_loader里面要一个一个数据取，用for\n",
    "        test_x,test_y=test_x.to(device),test_y.to(device)\n",
    "        \n",
    "        out=network(test_x)\n",
    "        pred=out.argmax(dim=1)\n",
    "        \n",
    "        total += test_y.size(0)\n",
    "        sum_correct += (pred.cpu().data.numpy()==test_y.cpu().data.numpy()).sum().item()\n",
    "        \n",
    "    my_rate=sum_correct/total\n",
    "    print('By using the test set,the prediction rate is: ',my_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "labels=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n",
      "(50000, 3, 32, 32)\n",
      "(50000,)\n",
      "(50000, 3, 32, 32)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# load data，另一种不从torchvision.datasets中读取数据集，而是直接通过函数读取数据的方法 \n",
    "# (但是这样就不能用torchvision自带的数据增强和标准化)\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb')as f:\n",
    "        datadict = pickle.load(f,encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def unpickle_labels(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "#加载标签名字\n",
    "labels = unpickle_labels(\"./data/batches.meta\")\n",
    "print(labels[b'label_names'])\n",
    "\n",
    "#加载训练集\n",
    "train_X=[]\n",
    "train_Y=[]\n",
    "for i in range(5):\n",
    "    s = './data/data_batch_' + str(i+1)\n",
    "    train_X1, train_Y1 = load_CIFAR_batch(\"./data/data_batch_1\")\n",
    "    train_X.append(train_X1)\n",
    "    train_Y.append(train_Y1)\n",
    "    \n",
    "train_X = np.concatenate((train_X[0],train_X[1],train_X[2],train_X[3],train_X[4]),axis=0)\n",
    "train_Y = np.concatenate((train_Y[0],train_Y[1],train_Y[2],train_Y[3],train_Y[4]),axis=0)   \n",
    "\n",
    "#加载验证集\n",
    "test_X,test_Y = load_CIFAR_batch('./data/test_batch')\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "# 我现在的电脑内存不够，先尝试减少训练集，并且随机化输入\n",
    "indices = np.random.choice(50000,50000)   #从50000个数据中抽取50000个数据\n",
    "train_X = train_X[indices]   \n",
    "train_Y = train_Y[indices]\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "\n",
    "# 用这个方法读取数据的验证集的验证流程：\n",
    "pred_Y = network(test_X.to(device))\n",
    "pred_Y = np.argmax(pred_Y.cpu().data.numpy(),axis=1)\n",
    "\n",
    "print(pred_Y.shape)\n",
    "\n",
    "accurate = (pred_Y== test_Y.cpu().data.numpy()).sum().item()/test_Y.shape[0]\n",
    "print(accurate)\n",
    "print(pred_Y[:100])\n",
    "print(test_Y[:100])\n",
    "\n",
    "#print(test_Y[:10])\n",
    "#pred_Y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
