{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import gym              # this includes many visualisable physical experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000\n",
    "\n",
    "env = gym.make('CartPole-v0')               # import experiment\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n                # N actions of the ex\n",
    "N_STATES = env.observation_space.shape[0]       # N observation points(N states) of the ex\n",
    "\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):      #输入是N_STATES,输出是N_ACTIONS,在当前N个state下采取最好的一个action\n",
    "    def __init__(self,):   \n",
    "        super(Net,self).__init__() \n",
    "        self.fc1 = nn.Linear(N_STATES, 50)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization #随机初始化权重的过程，按照normalize，平均值0，标准差0.1\n",
    "        self.out = nn.Linear(50, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "    \n",
    "class DQN(object):                  # DQN framework\n",
    "    def __init__(self,):\n",
    "        self.eval_net, self.target_net = Net(), Net()   # eval_net时刻更新学习，target_net每定期多少步后用eval_net来更新\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating， 通过这个定义每一百步更新一次target_net\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory # N_STATES * 2为s,s_的存储，+2为a和r的存储\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "    def choose_action(self,x):           # x: observation state\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # 这里只输入一个 sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy，表示贪心的时候：选最优动作\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()        # return the argmax\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        else:   # non greedy: 选随机动作\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self,s,a,r,s_):               # r: reward, s_: next state   # 该函数存储记忆\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # 如果记忆库满了, 就覆盖老数据\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def learn(self):\n",
    "        # target net 参数更新,用load_state_dict直接从eval_net中更新，而不从数据中更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())        \n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # 随机抽取记忆库中的批数据（批量32）\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE) #表示取BATCH_SIZE个数，每个数随机值，范围0到MEMORY_CAPACITY-1\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))    #shape (batch,1)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])             #shape (batch,1)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)\n",
    "        #算q_eval,这是预测值\n",
    "        q_eval = self.eval_net(b_s).gather(dim=1, index=b_a)  # shape (batch, 1) #self.eval_net(b_s)的输出为actions_value\n",
    "        #算q_target，这是现实值\n",
    "        q_next = self.target_net(b_s_).detach()     # q_next 不进行反向传递误差,因为tagrt_net这里不更新, 所以 detach\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        #计算loss\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        \n",
    "        #上面这一部分更新我的理解：首先我们知道了Qlearning是用的类似q_target的方法迭代，并且能迭代到准确的q值，在DQL中，我们用s_的数据\n",
    "        #（表示下一步的数据）来更新q_target，用s来更新eval（表示上一步的数据，并且eval是用神经网络训练的），表示eval追赶target的过程\n",
    "\n",
    "        # 计算, 更新 eval net\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n",
      "Ep:  206 | Ep_r:  2.54\n",
      "Ep:  207 | Ep_r:  1.53\n",
      "Ep:  208 | Ep_r:  1.34\n",
      "Ep:  209 | Ep_r:  1.21\n",
      "Ep:  210 | Ep_r:  2.59\n",
      "Ep:  211 | Ep_r:  2.8\n",
      "Ep:  212 | Ep_r:  2.32\n",
      "Ep:  213 | Ep_r:  3.24\n",
      "Ep:  214 | Ep_r:  2.02\n",
      "Ep:  215 | Ep_r:  2.87\n",
      "Ep:  216 | Ep_r:  3.79\n",
      "Ep:  217 | Ep_r:  2.34\n",
      "Ep:  218 | Ep_r:  3.32\n",
      "Ep:  219 | Ep_r:  10.17\n",
      "Ep:  220 | Ep_r:  2.27\n",
      "Ep:  221 | Ep_r:  1.83\n",
      "Ep:  222 | Ep_r:  2.11\n",
      "Ep:  223 | Ep_r:  1.05\n",
      "Ep:  224 | Ep_r:  1.61\n",
      "Ep:  225 | Ep_r:  1.31\n",
      "Ep:  226 | Ep_r:  2.2\n",
      "Ep:  227 | Ep_r:  2.97\n",
      "Ep:  228 | Ep_r:  2.08\n",
      "Ep:  229 | Ep_r:  3.74\n",
      "Ep:  230 | Ep_r:  3.54\n",
      "Ep:  231 | Ep_r:  2.06\n",
      "Ep:  232 | Ep_r:  0.27\n",
      "Ep:  233 | Ep_r:  25.56\n",
      "Ep:  234 | Ep_r:  41.18\n",
      "Ep:  235 | Ep_r:  28.19\n",
      "Ep:  236 | Ep_r:  45.14\n",
      "Ep:  237 | Ep_r:  15.39\n",
      "Ep:  238 | Ep_r:  0.17\n",
      "Ep:  239 | Ep_r:  54.01\n",
      "Ep:  240 | Ep_r:  55.74\n",
      "Ep:  241 | Ep_r:  137.35\n",
      "Ep:  242 | Ep_r:  140.44\n",
      "Ep:  243 | Ep_r:  157.37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-13dc3befa5cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mep_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# 显示实验动画\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m#根据当前state来选取action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training process\n",
    "dqn = DQN() # 定义 DQN 系统\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(400): \n",
    "    s = env.reset()         # initialize\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()        # 显示实验动画\n",
    "        a = dqn.choose_action(s)            #根据当前state来选取action\n",
    "\n",
    "        # 执行动作a, 得到环境反馈\n",
    "        s_, r, done, info = env.step(a) #done表示是否要环境重置 env.reset，当 Done 为 True 时，就表明当前回合(episode)或者试验(tial)结束。\n",
    "                                        #例如当机器人摔倒或者掉出台面，就应当终止当前回合进行重置(reset)\n",
    "\n",
    "        # 修改 reward, 使 DQN 快速学习，表示小车杆子越直，reward越大，小车越在中间，reward越大\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "        \n",
    "        # 存记忆\n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "\n",
    "        ep_r += r\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()          # 记忆库满了就进行学习)\n",
    "            if done:       \n",
    "                print('Ep: ', i_episode,\n",
    "                      '| Ep_r: ', round(ep_r, 2))\n",
    "        \n",
    "        if done:               #如果回合结束, 进入下回合\n",
    "            break\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 4.]])\n",
      "tensor([[1., 5., 6.],\n",
      "        [1., 2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.gather 函数\n",
    "b = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "print(b)\n",
    "index_1 = torch.LongTensor([[0,1],[2,0]])\n",
    "index_2 = torch.LongTensor([[0,1,1],[0,0,0]])\n",
    "print(torch.gather(b, dim=1, index=index_1))\n",
    "print(torch.gather(b, dim=0, index=index_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
